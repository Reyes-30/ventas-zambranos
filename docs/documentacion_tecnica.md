# üìã Documentaci√≥n T√©cnica - Sistema de An√°lisis de Ventas Zambranos

## üèóÔ∏è Arquitectura del Sistema

### Visi√≥n General

El Sistema de An√°lisis de Ventas Zambranos es una aplicaci√≥n web moderna construida con **Streamlit** que proporciona capacidades avanzadas de an√°lisis de datos, machine learning y visualizaci√≥n. La arquitectura sigue un patr√≥n modular que separa responsabilidades y facilita el mantenimiento.

```
Sistema de An√°lisis Zambranos/
‚îú‚îÄ‚îÄ app.py                    # Aplicaci√≥n principal Streamlit
‚îú‚îÄ‚îÄ src/app/                  # M√≥dulos de negocio
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ io_utils.py          # Manejo de archivos y validaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ processing.py        # L√≥gica de an√°lisis y ML
‚îÇ   ‚îú‚îÄ‚îÄ exceptions.py        # Jerarqu√≠a de excepciones
‚îÇ   ‚îî‚îÄ‚îÄ tour_interactivo.py  # Sistema de tour guiado
‚îú‚îÄ‚îÄ data/uploads/            # Almacenamiento seguro de archivos
‚îú‚îÄ‚îÄ logs/                    # Sistema de logging
‚îú‚îÄ‚îÄ tests/                   # Suite de pruebas
‚îî‚îÄ‚îÄ docs/                   # Documentaci√≥n
```

### Principios de Dise√±o

1. **Separaci√≥n de Responsabilidades**: Cada m√≥dulo tiene una funci√≥n espec√≠fica
2. **Seguridad Primero**: Validaci√≥n exhaustiva y manejo seguro de archivos
3. **Experiencia de Usuario**: Tour interactivo y ayuda contextual
4. **Escalabilidad**: Arquitectura modular para futuras extensiones
5. **Mantenibilidad**: C√≥digo bien documentado y probado

---

## üîß Stack Tecnol√≥gico

### Core Framework
- **Streamlit 1.37+**: Framework web para aplicaciones de datos
- **Python 3.8+**: Lenguaje base del sistema

### Procesamiento de Datos
- **pandas 2.0+**: Manipulaci√≥n y an√°lisis de datos
- **numpy 1.24+**: Operaciones num√©ricas y arrays
- **scikit-learn 1.3+**: Algoritmos de machine learning

### Visualizaci√≥n
- **plotly 5.15+**: Gr√°ficas interactivas y din√°micas
- **matplotlib 3.7+**: Gr√°ficas est√°ticas de alta calidad
- **seaborn 0.12+**: Visualizaciones estad√≠sticas avanzadas
- **kaleido 0.2+**: Exportaci√≥n de gr√°ficas Plotly a PNG

### Documentaci√≥n y Reportes
- **python-docx 0.8+**: Generaci√≥n de documentos Word
- **streamlit-guided-tour**: Tour interactivo de usuario

### Desarrollo y Testing
- **pytest 7.4+**: Framework de pruebas unitarias
- **pytest-benchmark**: Pruebas de rendimiento
- **filelock 3.12+**: Sincronizaci√≥n de archivos

### Logging y Monitoreo
- **logging (built-in)**: Sistema de logs estructurado
- **hashlib (built-in)**: Hashing SHA-256 para archivos

---

## üìä Modelo de Datos

### Esquema de Entrada

El sistema espera datos tabulares con el siguiente esquema m√≠nimo:

```python
REQUIRED_COLUMNS = [
    'Mes',              # str: Nombre del mes en espa√±ol
    'Categor√≠a',        # str: Categor√≠a del producto
    'Cantidad Vendida', # int/float: Unidades vendidas
    'Ingreso Total',    # float: Ingresos totales
    'ISV',              # float: Impuesto sobre ventas
    'Utilidad Bruta'    # float: Ganancia bruta
]

OPTIONAL_COLUMNS = [
    'Precio Unitario',  # float: Precio por unidad
    'Costo Unitario',   # float: Costo por unidad
    'Costo Total',      # float: Costo total
    'Ingreso Neto'      # float: Ingreso despu√©s de costos
]
```

### Transformaciones de Datos

#### 1. Validaci√≥n y Coerci√≥n
```python
def validate_schema(df: pd.DataFrame) -> tuple[bool, list]:
    """Valida esquema m√≠nimo de datos"""
    missing = [col for col in REQUIRED_COLUMNS if col not in df.columns]
    return len(missing) == 0, missing

def coerce_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:
    """Convierte columnas a tipos num√©ricos"""
    numeric_cols = ['Cantidad Vendida', 'Ingreso Total', 'ISV', 'Utilidad Bruta']
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    return df
```

#### 2. Normalizaci√≥n Temporal
```python
MONTH_ORDER = [
    'Enero', 'Febrero', 'Marzo', 'Abril', 'Mayo', 'Junio',
    'Julio', 'Agosto', 'Septiembre', 'Octubre', 'Noviembre', 'Diciembre'
]

def normalize_months(df: pd.DataFrame) -> pd.DataFrame:
    """Ordena meses cronol√≥gicamente"""
    df['Mes'] = pd.Categorical(df['Mes'], categories=MONTH_ORDER, ordered=True)
    return df.sort_values('Mes')
```

---

## üèóÔ∏è Componentes del Sistema

### 1. `app.py` - Aplicaci√≥n Principal

#### Responsabilidades
- Interfaz de usuario Streamlit
- Coordinaci√≥n entre m√≥dulos
- Gesti√≥n de estado de sesi√≥n
- Control de flujo de la aplicaci√≥n

#### Estructura Principal
```python
def main():
    """Funci√≥n principal de la aplicaci√≥n"""
    configure_page()           # Configuraci√≥n de p√°gina
    load_custom_css()         # Estilos personalizados
    create_sidebar()          # Panel de control lateral
    
    if 'df' in st.session_state:
        show_main_tabs()      # Tabs de an√°lisis
    else:
        show_upload_interface()  # Interfaz de carga
```

#### Gesti√≥n de Estado
```python
# Variables de sesi√≥n cr√≠ticas
st.session_state = {
    'df': pd.DataFrame,           # Datos principales
    'theme': str,                 # Tema visual
    'interactive_plots': bool,    # Modo de gr√°ficas
    'saved_plots': dict,         # Cache de gr√°ficas
    'tour_progress': dict,       # Progreso del tour
    'file_hash': str            # Hash del archivo actual
}
```

### 2. `src/app/io_utils.py` - Gesti√≥n de Archivos

#### Funciones Principales

```python
def read_dataframe(file_obj, filename: str, delimiter: str = 'auto') -> pd.DataFrame:
    """Lee archivo CSV/Excel con detecci√≥n autom√°tica de formato"""
    
def save_upload_safely(uploaded_file) -> tuple[str, str]:
    """Guarda archivo de forma segura con hash √∫nico"""
    
def list_recent_files() -> list[tuple[str, str, datetime]]:
    """Lista archivos recientes con metadatos"""
    
def generate_file_hash(content: bytes) -> str:
    """Genera hash SHA-256 para identificaci√≥n √∫nica"""
```

#### Seguridad de Archivos
- **Atomic Writes**: Escritura at√≥mica para evitar corrupci√≥n
- **File Locking**: `filelock` para acceso concurrente seguro
- **Hash-based Naming**: Evita colisiones y sobreescritura accidental
- **Validation**: Verificaci√≥n de formato y tama√±o

### 3. `src/app/processing.py` - L√≥gica de An√°lisis

#### M√©tricas Estad√≠sticas
```python
@dataclass
class MetricasResumen:
    """Estructura de m√©tricas principales"""
    ingresos_totales: float
    isv_total: float
    utilidad_promedio: float
    unidades_vendidas: int
    num_categorias: int
    meses_activos: int

def resumen_metricas(df: pd.DataFrame) -> MetricasResumen:
    """Calcula m√©tricas ejecutivas principales"""
```

#### Machine Learning
```python
@dataclass
class PCAResult:
    """Resultado del an√°lisis PCA"""
    components: np.ndarray
    explained_variance_ratio: np.ndarray
    transformed_data: np.ndarray

@dataclass
class ClusterResult:
    """Resultado del clustering K-Means"""
    labels: np.ndarray
    centers: np.ndarray
    silhouette_score: float
    cluster_profiles: pd.DataFrame

def run_pca(df: pd.DataFrame, n_components: int = 2) -> PCAResult:
    """Ejecuta an√°lisis de componentes principales"""
    
def run_kmeans(df: pd.DataFrame, n_clusters: int = 3) -> ClusterResult:
    """Ejecuta clustering K-Means"""
```

### 4. `src/app/exceptions.py` - Manejo de Errores

#### Jerarqu√≠a de Excepciones
```python
class ZambranosAnalysisError(Exception):
    """Excepci√≥n base del sistema"""

class DataValidationError(ZambranosAnalysisError):
    """Errores de validaci√≥n de datos"""

class FileProcessingError(ZambranosAnalysisError):
    """Errores de procesamiento de archivos"""

class AnalysisError(ZambranosAnalysisError):
    """Errores durante an√°lisis estad√≠stico"""

class MLError(ZambranosAnalysisError):
    """Errores en algoritmos de ML"""
```

### 5. `src/app/tour_interactivo.py` - Sistema de Tour

#### Arquitectura del Tour
```python
@dataclass
class PasoTour:
    """Definici√≥n de un paso del tour"""
    titulo: str
    contenido: str
    posicion: str  # sidebar, main, tab
    tab_objetivo: str = None
    accion_requerida: str = None

def crear_pasos_tour() -> list[PasoTour]:
    """Define secuencia completa del tour (12 pasos)"""
    
def mostrar_tour_interactivo():
    """Ejecuta tour guiado con navegaci√≥n"""
```

---

## üé® Sistema de UI/UX

### Themes y Estilos

#### CSS Personalizado
```css
/* Tema Moderno */
.metric-card {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    border-radius: 15px;
    padding: 20px;
    color: white;
    box-shadow: 0 8px 32px rgba(0,0,0,0.1);
}

/* Tema Cl√°sico */
.classic-theme {
    --primary-color: #2E86AB;
    --secondary-color: #A23B72;
    --background-color: #F18F01;
}

/* Tema Oscuro */
.dark-theme {
    --bg-color: #1E1E1E;
    --text-color: #FFFFFF;
    --accent-color: #FF6B6B;
}
```

#### Sistema de Componentes
```python
def create_metric_card(title: str, value: str, delta: str = None):
    """Crea tarjeta de m√©trica estilizada"""
    
def create_styled_dataframe(df: pd.DataFrame, height: int = 400):
    """DataFrame con estilos personalizados"""
    
def show_success_message(message: str):
    """Mensaje de √©xito con √≠conos"""
```

### Responsive Design

El sistema adapta autom√°ticamente el layout seg√∫n el dispositivo:
- **Desktop**: Layout completo con sidebar
- **Tablet**: Sidebar colapsable
- **Mobile**: Stack vertical de componentes

---

## üîç Algoritmos de An√°lisis

### An√°lisis Estad√≠stico

#### Correlaciones
```python
def calcular_correlaciones(df: pd.DataFrame) -> tuple[pd.DataFrame, list]:
    """
    Calcula matriz de correlaci√≥n y identifica correlaciones fuertes
    
    Returns:
        tuple: (correlation_matrix, strong_correlations)
    """
    numeric_df = df.select_dtypes(include=[np.number])
    correlation_matrix = numeric_df.corr()
    
    # Identificar correlaciones fuertes
    strong_corr = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            corr_value = correlation_matrix.iloc[i, j]
            if abs(corr_value) > 0.7:  # Umbral de correlaci√≥n fuerte
                strong_corr.append({
                    'var1': correlation_matrix.columns[i],
                    'var2': correlation_matrix.columns[j],
                    'correlation': corr_value
                })
    
    return correlation_matrix, strong_corr
```

#### Detecci√≥n de Outliers
```python
def detectar_outliers(df: pd.DataFrame, column: str) -> dict:
    """
    Detecta outliers usando m√©todo IQR
    
    Returns:
        dict: Informaci√≥n detallada de outliers
    """
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    
    return {
        'count': len(outliers),
        'percentage': (len(outliers) / len(df)) * 100,
        'bounds': (lower_bound, upper_bound),
        'outlier_values': outliers[column].tolist()
    }
```

### Machine Learning

#### PCA - An√°lisis de Componentes Principales
```python
def run_pca(df: pd.DataFrame, n_components: int = 2) -> PCAResult:
    """
    Ejecuta PCA para reducci√≥n dimensional
    
    Algorithm:
    1. Standardize data (Z-score normalization)
    2. Compute covariance matrix
    3. Find eigenvalues and eigenvectors
    4. Project data onto principal components
    """
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler
    
    # Preparaci√≥n de datos
    numeric_df = df.select_dtypes(include=[np.number])
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(numeric_df)
    
    # PCA
    pca = PCA(n_components=n_components)
    transformed_data = pca.fit_transform(scaled_data)
    
    return PCAResult(
        components=pca.components_,
        explained_variance_ratio=pca.explained_variance_ratio_,
        transformed_data=transformed_data
    )
```

#### K-Means Clustering
```python
def run_kmeans(df: pd.DataFrame, n_clusters: int = 3) -> ClusterResult:
    """
    Ejecuta clustering K-Means
    
    Algorithm:
    1. Initialize k centroids randomly
    2. Assign points to nearest centroid
    3. Update centroids to cluster means
    4. Repeat until convergence
    """
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score
    from sklearn.preprocessing import StandardScaler
    
    # Preparaci√≥n
    numeric_df = df.select_dtypes(include=[np.number])
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(numeric_df)
    
    # K-Means
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(scaled_data)
    
    # M√©tricas de calidad
    silhouette_avg = silhouette_score(scaled_data, labels)
    
    # Perfil de clusters
    cluster_profiles = calculate_cluster_profiles(df, labels)
    
    return ClusterResult(
        labels=labels,
        centers=kmeans.cluster_centers_,
        silhouette_score=silhouette_avg,
        cluster_profiles=cluster_profiles
    )
```

---

## üìä Visualizaci√≥n y Gr√°ficas

### Sistema Dual de Visualizaci√≥n

#### Plotly (Interactivo)
```python
def create_interactive_plot(df: pd.DataFrame, plot_type: str, **kwargs):
    """Crea gr√°ficas interactivas con Plotly"""
    
    if plot_type == 'bar':
        fig = px.bar(df, **kwargs)
        fig.update_layout(
            template="plotly_white",
            hovermode="x unified",
            showlegend=True
        )
    elif plot_type == 'line':
        fig = px.line(df, **kwargs)
        fig.update_traces(mode='lines+markers')
    
    return fig
```

#### Matplotlib (Est√°tico)
```python
def create_static_plot(df: pd.DataFrame, plot_type: str, **kwargs):
    """Crea gr√°ficas est√°ticas con Matplotlib"""
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    if plot_type == 'bar':
        bars = ax.bar(df.index, df.values, **kwargs)
        # A√±adir valores en barras
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:,.0f}', ha='center', va='bottom')
    
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    return fig
```

### Sistema de Exportaci√≥n

#### PNG de Alta Resoluci√≥n
```python
def export_plot_png(fig, filename: str, dpi: int = 300):
    """Exporta gr√°fica como PNG de alta calidad"""
    
    if hasattr(fig, 'write_image'):  # Plotly
        fig.write_image(filename, width=1200, height=800, scale=2)
    else:  # Matplotlib
        fig.savefig(filename, dpi=dpi, bbox_inches='tight', 
                   facecolor='white', edgecolor='none')
```

---

## üîê Seguridad y Validaci√≥n

### Validaci√≥n de Archivos

#### Validaci√≥n de Formato
```python
def validate_file_format(filename: str) -> bool:
    """Valida formatos de archivo permitidos"""
    allowed_extensions = {'.csv', '.xlsx', '.xls'}
    return Path(filename).suffix.lower() in allowed_extensions

def validate_file_size(file_obj, max_size_mb: int = 50) -> bool:
    """Valida tama√±o m√°ximo de archivo"""
    file_obj.seek(0, 2)  # Ir al final
    size = file_obj.tell()
    file_obj.seek(0)     # Volver al inicio
    return size <= max_size_mb * 1024 * 1024
```

#### Sanitizaci√≥n de Datos
```python
def sanitize_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """Limpia y sanitiza DataFrame"""
    
    # Remover filas completamente vac√≠as
    df = df.dropna(how='all')
    
    # Limpiar espacios en strings
    string_cols = df.select_dtypes(include=['object']).columns
    df[string_cols] = df[string_cols].apply(lambda x: x.str.strip() if x.dtype == "object" else x)
    
    # Validar rangos num√©ricos
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        # Remover valores infinitos
        df[col] = df[col].replace([np.inf, -np.inf], np.nan)
        # Validar rangos razonables para datos de ventas
        if 'Precio' in col or 'Costo' in col:
            df[col] = df[col].where(df[col] >= 0)  # No negativos
    
    return df
```

### Logging y Monitoreo

#### Sistema de Logs Estructurado
```python
import logging
from datetime import datetime

# Configuraci√≥n de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/app.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

def log_user_action(action: str, details: dict = None):
    """Registra acciones del usuario"""
    log_entry = {
        'timestamp': datetime.now().isoformat(),
        'action': action,
        'details': details or {},
        'session_id': st.session_state.get('session_id', 'unknown')
    }
    logger.info(f"USER_ACTION: {log_entry}")

def log_error(error: Exception, context: str = ""):
    """Registra errores con contexto"""
    logger.error(f"ERROR in {context}: {type(error).__name__}: {str(error)}")
```

---

## üß™ Testing y Quality Assurance

### Estructura de Tests

```
tests/
‚îú‚îÄ‚îÄ test_io_utils.py          # Tests de manejo de archivos
‚îú‚îÄ‚îÄ test_processing.py        # Tests de an√°lisis
‚îú‚îÄ‚îÄ test_ml_algorithms.py     # Tests de ML
‚îú‚îÄ‚îÄ test_visualizations.py    # Tests de gr√°ficas
‚îú‚îÄ‚îÄ fixtures/                 # Datos de prueba
‚îÇ   ‚îú‚îÄ‚îÄ sample_data.csv
‚îÇ   ‚îî‚îÄ‚îÄ invalid_data.csv
‚îî‚îÄ‚îÄ conftest.py              # Configuraci√≥n pytest
```

### Tests Unitarios

#### Test de Validaci√≥n de Datos
```python
import pytest
import pandas as pd
from src.app.io_utils import validate_schema, coerce_numeric_columns

def test_validate_schema_valid_data():
    """Test validaci√≥n con datos v√°lidos"""
    df = pd.DataFrame({
        'Mes': ['Enero', 'Febrero'],
        'Categor√≠a': ['A', 'B'],
        'Cantidad Vendida': [100, 200],
        'Ingreso Total': [1000.0, 2000.0],
        'ISV': [150.0, 300.0],
        'Utilidad Bruta': [500.0, 1000.0]
    })
    
    is_valid, missing = validate_schema(df)
    assert is_valid == True
    assert missing == []

def test_validate_schema_missing_columns():
    """Test validaci√≥n con columnas faltantes"""
    df = pd.DataFrame({
        'Mes': ['Enero'],
        'Categor√≠a': ['A']
        # Faltan columnas requeridas
    })
    
    is_valid, missing = validate_schema(df)
    assert is_valid == False
    assert 'Cantidad Vendida' in missing
```

#### Test de Machine Learning
```python
def test_pca_basic_functionality():
    """Test b√°sico del algoritmo PCA"""
    df = create_sample_numeric_data()
    
    result = run_pca(df, n_components=2)
    
    assert result.transformed_data.shape[1] == 2
    assert len(result.explained_variance_ratio) == 2
    assert 0 <= result.explained_variance_ratio.sum() <= 1

def test_kmeans_clustering():
    """Test b√°sico del clustering K-Means"""
    df = create_sample_numeric_data()
    
    result = run_kmeans(df, n_clusters=3)
    
    assert len(set(result.labels)) <= 3  # M√°ximo 3 clusters
    assert -1 <= result.silhouette_score <= 1  # Rango v√°lido
    assert result.cluster_profiles.shape[0] == 3  # Un perfil por cluster
```

### Tests de Performance

```python
import pytest_benchmark

def test_processing_performance(benchmark):
    """Test de rendimiento para procesamiento de datos"""
    df = create_large_dataset(10000)  # 10k filas
    
    result = benchmark(resumen_metricas, df)
    
    assert result.ingresos_totales > 0
    # Benchmark se registra autom√°ticamente

def test_pca_performance(benchmark):
    """Test de rendimiento para PCA"""
    df = create_large_numeric_dataset(5000)
    
    result = benchmark(run_pca, df, n_components=3)
    
    assert result.transformed_data.shape[0] == 5000
```

### Continuous Integration

#### GitHub Actions (Ejemplo)
```yaml
name: CI/CD Pipeline

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-benchmark
    
    - name: Run tests
      run: |
        pytest tests/ -v --benchmark-skip
    
    - name: Run performance tests
      run: |
        pytest tests/ --benchmark-only --benchmark-json=benchmark.json
```

---

## üöÄ Deployment y DevOps

### Configuraci√≥n de Producci√≥n

#### Dockerfile
```dockerfile
FROM python:3.9-slim

WORKDIR /app

# Instalar dependencias del sistema
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Copiar requerimientos e instalar
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copiar c√≥digo
COPY . .

# Crear directorios necesarios
RUN mkdir -p data/uploads logs

# Exponer puerto
EXPOSE 8501

# Comando de inicio
CMD ["streamlit", "run", "app.py", "--server.address", "0.0.0.0"]
```

#### docker-compose.yml
```yaml
version: '3.8'

services:
  zambranos-app:
    build: .
    ports:
      - "8501:8501"
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    environment:
      - STREAMLIT_SERVER_HEADLESS=true
      - STREAMLIT_SERVER_ENABLE_CORS=false
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - zambranos-app
```

### Monitoreo en Producci√≥n

#### Health Checks
```python
def health_check():
    """Endpoint de health check"""
    checks = {
        'database': check_data_directory(),
        'logging': check_log_system(),
        'memory': check_memory_usage(),
        'disk': check_disk_space()
    }
    
    all_healthy = all(checks.values())
    
    return {
        'status': 'healthy' if all_healthy else 'unhealthy',
        'checks': checks,
        'timestamp': datetime.now().isoformat()
    }
```

#### Metrics Collection
```python
def collect_usage_metrics():
    """Recolecta m√©tricas de uso"""
    return {
        'active_sessions': count_active_sessions(),
        'files_processed_today': count_daily_uploads(),
        'avg_processing_time': get_avg_processing_time(),
        'memory_usage': get_memory_usage(),
        'error_rate': calculate_error_rate()
    }
```

---

## üîß Mantenimiento y Extensibilidad

### Patrones de Extensi√≥n

#### A√±adir Nuevos An√°lisis
```python
# 1. Definir dataclass para resultado
@dataclass
class NuevoAnalisisResult:
    resultado_principal: float
    datos_adicionales: dict

# 2. Implementar funci√≥n de an√°lisis
def nuevo_analisis(df: pd.DataFrame, parametros: dict) -> NuevoAnalisisResult:
    """Nueva funcionalidad de an√°lisis"""
    # L√≥gica de an√°lisis aqu√≠
    pass

# 3. A√±adir al pipeline principal
def run_all_analyses(df: pd.DataFrame):
    """Pipeline completo de an√°lisis"""
    return {
        'metricas': resumen_metricas(df),
        'pca': run_pca(df),
        'clustering': run_kmeans(df),
        'nuevo_analisis': nuevo_analisis(df, {})  # ‚Üê Nuevo an√°lisis
    }
```

#### Nuevos Tipos de Visualizaci√≥n
```python
def register_new_plot_type(plot_type: str, render_function):
    """Registra nuevo tipo de gr√°fica"""
    PLOT_RENDERERS[plot_type] = render_function

def create_custom_visualization(df: pd.DataFrame, viz_type: str, **kwargs):
    """Factory para visualizaciones personalizadas"""
    if viz_type in PLOT_RENDERERS:
        return PLOT_RENDERERS[viz_type](df, **kwargs)
    else:
        raise ValueError(f"Tipo de visualizaci√≥n no soportado: {viz_type}")
```

### Database Integration (Futuro)

#### Estructura para Base de Datos
```python
# models.py
from sqlalchemy import Column, Integer, String, Float, DateTime
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class AnalysisSession(Base):
    __tablename__ = 'analysis_sessions'
    
    id = Column(Integer, primary_key=True)
    session_id = Column(String(64), unique=True)
    file_hash = Column(String(64))
    created_at = Column(DateTime)
    last_accessed = Column(DateTime)

class ProcessedData(Base):
    __tablename__ = 'processed_data'
    
    id = Column(Integer, primary_key=True)
    session_id = Column(String(64))
    analysis_type = Column(String(32))
    result_data = Column(String)  # JSON serialized
    created_at = Column(DateTime)
```

### API Integration (Futuro)

#### REST API Endpoints
```python
from fastapi import FastAPI, UploadFile
from pydantic import BaseModel

app = FastAPI()

class AnalysisRequest(BaseModel):
    analysis_type: str
    parameters: dict

@app.post("/api/upload")
async def upload_data(file: UploadFile):
    """Endpoint para subir datos"""
    pass

@app.post("/api/analyze")
async def run_analysis(request: AnalysisRequest):
    """Endpoint para ejecutar an√°lisis"""
    pass

@app.get("/api/results/{session_id}")
async def get_results(session_id: str):
    """Endpoint para obtener resultados"""
    pass
```

---

## üìö Referencias y Recursos

### Documentaci√≥n Externa
- [Streamlit Documentation](https://docs.streamlit.io/)
- [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/)
- [Scikit-learn Documentation](https://scikit-learn.org/stable/user_guide.html)
- [Plotly Python Documentation](https://plotly.com/python/)

### Libros Recomendados
- "Python for Data Analysis" by Wes McKinney
- "Hands-On Machine Learning" by Aur√©lien G√©ron
- "Streamlit for Data Science" by Tyler Richards

### Papers y Art√≠culos
- "Principal Component Analysis: A Review" (Jolliffe, 2016)
- "k-means++: The Advantages of Careful Seeding" (Arthur & Vassilvitskii, 2007)

---

## üìù Changelog y Versiones

### Versi√≥n 2.0 (Actual)
- ‚úÖ Sistema de tour interactivo
- ‚úÖ Arquitectura modular completa
- ‚úÖ Suite de testing con pytest
- ‚úÖ Documentaci√≥n t√©cnica completa
- ‚úÖ Sistema de logging estructurado

### Versi√≥n 1.5
- ‚úÖ Machine Learning (PCA + K-Means)
- ‚úÖ Sistema de exportaciones
- ‚úÖ Validaci√≥n de archivos
- ‚úÖ M√∫ltiples temas visuales

### Versi√≥n 1.0
- ‚úÖ Aplicaci√≥n base Streamlit
- ‚úÖ An√°lisis estad√≠stico b√°sico
- ‚úÖ Visualizaciones con Plotly/Matplotlib
- ‚úÖ Carga de archivos CSV/Excel

### Roadmap Futuro
- üîÑ Integraci√≥n con bases de datos
- üîÑ API REST para integraciones
- üîÑ Sistema de usuarios y autenticaci√≥n
- üîÑ Dashboard en tiempo real
- üîÑ An√°lisis predictivo avanzado

---

*Documentaci√≥n T√©cnica v2.0*  
*√öltima actualizaci√≥n: Septiembre 2025*  
*Equipo de Desarrollo: Sistema Zambranos*
